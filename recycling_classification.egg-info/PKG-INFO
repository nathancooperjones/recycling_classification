Metadata-Version: 1.0
Name: recycling-classification
Version: 0.1.0
Summary: using deep learning to identify recyclable items
Home-page: UNKNOWN
Author: UNKNOWN
Author-email: UNKNOWN
License: UNKNOWN
Description: # Deep Learning Recycling Classification
        
        ![](https://www.laredosolidwaste.com/images/icons/FAQicon.jpg)
        
        Have you ever help a piece of trash and been paralyzed with the fear of the unknown: trash or recycling? If it's trash that ends up in recycling, the whole bin could be contaminated! If it's recyclables ending up in the trash, you missed a potential opportunity to save the planet. It is this endless confusion that brought the idea for recycling detection.
        
        Using `fastai`'s deep learning library, this is a small project made to determine types of recyclables, providing proper recycling instructions for each through a simple API.
        
        ### Development
        Run development from within the docker container
        ```bash
        export RECYCLE_DATA_PATH=$HOME/data/recycling_classification
        
        # build the image to extract features
        docker build -t recycling_classification .
        
        # run the container in interactive mode.
        docker run \
            -it \
            --rm \
            -v "${RECYCLE_DATA_PATH}:/data" \
            -v "${PWD}:/recycling_classification" \
            -p 8888:8888 \
            -p 80:80 \
            -e AWS_ACCESS_KEY_ID="$(aws --profile default configure get aws_access_key_id)" \
            -e AWS_SECRET_ACCESS_KEY="$(aws --profile default configure get aws_secret_access_key)" \
            recycling_classification /bin/bash -c "pip install -r requirements-dev.txt && bash"
        ```
        
        Note: If you want to override which model is downloaded, you can set the S3 path(s) for the object by passing an environment variable in with the `docker run` command, e.g.
        ```bash
            -e MODEL_FILENAME="other/url/here/new_model.pkl" \
        ```
        
        Before starting the app, download the desired model and mapping dictionary with:
        ```bash
        bash download_data.sh
        ```
        
        From within the container, either of the following two commands will start the app:
        ```bash
        # Debug mode with flask native serving
        python3 recycling_classification/app.py
        
        # Production mode with gunicorn serving
        bash run_app.sh
        ```
        
        In either setup, we can still make requests to the service with
        ```bash
        curl -X GET http://0.0.0.0:80/health
        ```
        
        ### Start Jupyter Lab
        To run jupyterlab, start the container and execute the following:
        ```bash
        jupyter lab --ip 0.0.0.0 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password=''
        ```
        Connect to jupyter here: [http://localhost:8888/tree?](http://localhost:8888/tree?)
        
Platform: UNKNOWN
